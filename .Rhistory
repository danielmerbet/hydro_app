data$tp <- NULL; data$cc <- NULL; data$sp <- NULL
data$rh <- NULL; data$strat <- NULL; data$sh <- NULL
data$lh <- NULL; data$sw; data$sm7
#train RF
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = data, ntree = 500)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, data[tvar]))^2,2) ; rsqOOB
importance_random <- importance(RFfit); importance_random
importance_random/sum(importance_random)*100
#merge all
data <- merge(drivers, target, by="date")
data$yday <- yday(data$date)
#ML Analysis
###############################################################
#Random forest
library(randomForest)
#remove some data thta may be not important
data$tp <- NULL; data$cc <- NULL; data$sp <- NULL
data$rh <- NULL; data$strat <- NULL; data$sh <- NULL
data$lh <- NULL; data$sw <- NULL; data$sm7 <-NULL
#train RF
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = data, ntree = 500)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, data[tvar]))^2,2) ; rsqOOB
importance_random <- importance(RFfit); importance_random
importance_random/sum(importance_random)*100
#stats are quite good, it seems promising
plot(data$date, data[tvar][,1])
points(data$date, predRF, col="red")
#HOWEVER
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
m <- 1:(dim(data)[1]*0.8)
traindata <- data[m,]
testdata <- data[-m,]
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, testdata[tvar]))^2,2) ; rsqOOB
importance_random <- importance(RFfit); importance_random
#now the stats are nos as good, but they are more realistic
plot(testdata$date, testdata[tvar][,1])
points(testdata$date, predRF, col="red")
rsqOOB = round((cor(predRF, testdata[tvar]))^2,2) ; rsqOOB
#now the stats are nos as good, but they are more realistic
plot(testdata$date, testdata[tvar][,1])
points(testdata$date, predRF, col="red")
#How about having training and testing being sampled randomly,
#they could be correlated in time and increase
#wrongly the predictability
m <- sample.int(n=nrow(data), size=floor(.8*nrow(data)),replace = F)
traindata <- data[m,]
testdata <- data[-m,]
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, testdata[tvar]))^2,2) ; rsqOOB
importance_random <- importance(RFfit); importance_random
importance_random <- importance(RFfit); importance_random
#HOWEVER
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
m <- 1:(dim(data)[1]*0.8)
traindata <- data[m,]
testdata <- data[-m,]
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test = round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
importance_random <- importance(RFfit); importance_random
importance_random/sum(importance_random)*100
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar] - predRF)^2))); rmse_test
testdata[tvar]
predRF
(testdata[tvar] - predRF)
(testdata[tvar] - predRF)^2
mean((testdata[tvar] - predRF)^2))
mean((testdata[tvar] - predRF)^2)
(testdata[tvar] - predRF)^2
sum((testdata[tvar] - predRF)^2)
mean((testdata[tvar] - predRF)^2)
class(testdata[tvar])
class(predRF)
class(testdata[tvar][,1])
rsqOOB = round((cor(predRF, testdata[tvar][,1]))^2,2) ; rsqOOB
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2))); rmse_test
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2))); rmse_test
importance_random <- importance(RFfit); importance_random
#now the stats are nos as good, but they are real (not affected by autocorrelation)
plot(testdata$date, testdata[tvar][,1])
points(testdata$date, predRF, col="red")
#now the stats are as good, but they are real (not affected by autocorrelation)
plot(testdata$date, testdata[tvar][,1])
points(testdata$date, predRF, col="red")
#How about having training and testing being sampled randomly,
#they could be autocorrelated in time and increase
#wrongly the predictability
m <- sample.int(n=nrow(data), size=floor(.8*nrow(data)),replace = F)
traindata <- data[m,]
testdata <- data[-m,]
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, testdata[tvar][,1]))^2,2) ; rsqOOB
###############################################################
#XGBoost
library(ggplot2)
library(xgboost)
traindatax <- as.matrix(traindata[, setdiff(colnames(traindata), c(tvar, "date"))])
#traindatax <- as.matrix(subset(traindata, select=-c(fdom,date)))  # Los predictores como matriz
traindatay <- traindata[tvar][,1]   # La variable objetivo como vector empezando por 0
# Convertir los datos de entrenamiento y prueba a xgb.DMatrix
#dtrain <- xgb.DMatrix(data = traindatax, label = traindatay)
dtrain <- xgb.DMatrix(data = traindatax, label = traindatay)
testdatax <- as.matrix(testdata[, setdiff(colnames(testdata), c(tvar, "date"))])
#testdatax<- as.matrix(subset(testdata, select=-c(doc,date)))
testdatay<- c(testdata[tvar][,1])
#dtest <- xgb.DMatrix(data = testdatax, label = testdatay)
dtest <- xgb.DMatrix(data = testdatax, label = testdatay)
# Se usa la función xgb.cv (no xgboost) para obtener por k-fold CV el nº
# óptimo de árboles y el mse.
# Se entrenan los k modelos para la k-fold CV y guarda las predicciones,
fitxgb <- xgb.cv(data = dtrain, nrounds = 1000, nfold = 5, verbose = FALSE,
early_stopping_rounds = 50, eval_metric = c("rmse"),
prediction = TRUE)
names(fitxgb) # vemos los objetos contenidos en fitxgb
head(fitxgb$evaluation_log,30) # evolución del error rmse
# error mínimo, utilizado para determinar el nº de árboles (optntrees) por VC
minrmse   = min(fitxgb$evaluation_log$test_rmse_mean); minrmse
optntrees  = which.min(fitxgb$evaluation_log$test_rmse_mean); optntrees # con hpps por defecto
# Entrenar el modelo final con el número óptimo de árboles
final_model <- xgboost(data = dtrain,
nrounds = optntrees,
objective = "reg:squarederror",
eval_metric = "rmse",
verbose = 0)
# Realizar predicciones sobre los datos de prueba sin redondeo
pred <- predict(final_model, testdatax)
# Calcular el coeficiente de determinación R^2
rsq <- round((cor(pred, testdatay))^2, 2)
print(paste("R-squared:", rsq))
plot(testdatay, pred)
#Split into non autocorrelated samples
m <- 1:(dim(data)[1]*0.8)
traindata <- data[m,]
testdata <- data[-m,]
traindatax <- as.matrix(traindata[, setdiff(colnames(traindata), c(tvar, "date"))])
#traindatax <- as.matrix(subset(traindata, select=-c(fdom,date)))  # Los predictores como matriz
traindatay <- traindata[tvar][,1]   # La variable objetivo como vector empezando por 0
# Convertir los datos de entrenamiento y prueba a xgb.DMatrix
#dtrain <- xgb.DMatrix(data = traindatax, label = traindatay)
dtrain <- xgb.DMatrix(data = traindatax, label = traindatay)
testdatax <- as.matrix(testdata[, setdiff(colnames(testdata), c(tvar, "date"))])
#testdatax<- as.matrix(subset(testdata, select=-c(doc,date)))
testdatay<- c(testdata[tvar][,1])
#dtest <- xgb.DMatrix(data = testdatax, label = testdatay)
dtest <- xgb.DMatrix(data = testdatax, label = testdatay)
# Se usa la función xgb.cv (no xgboost) para obtener por k-fold CV el nº
# óptimo de árboles y el mse.
# Se entrenan los k modelos para la k-fold CV y guarda las predicciones,
fitxgb <- xgb.cv(data = dtrain, nrounds = 1000, nfold = 5, verbose = FALSE,
early_stopping_rounds = 50, eval_metric = c("rmse"),
prediction = TRUE)
names(fitxgb) # vemos los objetos contenidos en fitxgb
head(fitxgb$evaluation_log,30) # evolución del error rmse
plot(fitxgb$evaluation_log$train_rmse_mean)
plot(fitxgb$evaluation_log$train_rmse_mean, type="l")
lines(fitxgb$evaluation_log$test_rmse_mean)
plot(fitxgb$evaluation_log$train_rmse_mean, type="l")
lines(fitxgb$evaluation_log$test_rmse_mean, col="red")
names(fitxgb) # vemos los objetos contenidos en fitxgb
head(fitxgb$evaluation_log,30) # evolución del error rmse
# error mínimo, utilizado para determinar el nº de árboles (optntrees) por VC
minrmse   = min(fitxgb$evaluation_log$test_rmse_mean); minrmse
optntrees  = which.min(fitxgb$evaluation_log$test_rmse_mean); optntrees # con hpps por defecto
# Entrenar el modelo final con el número óptimo de árboles
final_model <- xgboost(data = dtrain,
nrounds = optntrees,
objective = "reg:squarederror",
eval_metric = "rmse",
verbose = 0)
# Realizar predicciones sobre los datos de prueba sin redondeo
pred <- predict(final_model, testdatax)
# Calcular el coeficiente de determinación R^2
rsq <- round((cor(pred, testdatay))^2, 2)
print(paste("R-squared:", rsq))
plot(testdatay, pred)
testdatay
plot(pred)
points(testdatay, col="red")
plot(testdatay, pred)
plot(testdatay)
points(pred, col="red")
# Extract the RMSE values and round numbers
results <- as.data.frame(fitxgb$evaluation_log)
results$round <- 1:nrow(results)
# Plotting
ggplot(results, aes(x = iter, y = train_rmse_mean)) +
geom_line(color = "blue", size = 1) +
geom_line(aes(y = test_rmse_mean), color = "red", size = 1) +
labs(title = "XGBoost Cross-Validation RMSE",
x = "Boosting Round",
y = "Root Mean Squared Error (RMSE)",
color = "Legend") +
theme_minimal() +
scale_color_manual(values = c("Train RMSE" = "blue", "Test RMSE" = "red")) +
theme(legend.position = "bottom") +
guides(colour = guide_legend(override.aes = list(size = 3)))
optntrees
rm(list=ls(all=TRUE))
#Set Time Zone
Sys.setenv(TZ='UTC')
#Load Packages - may not use all - Note I did not delete ones I am not using here
library(lattice)
library(lubridate)
library(zoo)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(reshape2)
library(randomForest)
library(rpart)
library(rpart.plot)
dataML1822<-read.csv('~/Documents/intoDBP/ML_DOC_Feeagh/DailyGlenFee_1822.csv', sep=",")
str(dataML1822)
#format Date - note I call it Date2 - I keep the original Date
dataML1822$Date2 <- as.POSIXct(as.character(dataML1822$Date), format = "%Y-%m-%d")
str(dataML1822)
#a simple plot
plot(dataML1822$Date2, dataML1822$DOC_est_FEE, type="b", pch = 19, xlab = "", ylab = "estimated DOC mg/L")
#identifying missing driver data - just to know
#subset by year
NEWdataML2018<-subset(dataML1822, year==2018)
NEWdataML2019<-subset(dataML1822, year==2019)
#no2020
NEWdataML2021<-subset(dataML1822, year==2021)
NEWdataML2022<-subset(dataML1822, year==2022)
str(NEWdataML2018)
str(NEWdataML2019)
#n0 2020
str(NEWdataML2021)
str(NEWdataML2022)
summary(NEWdataML2018$DOC_est_FEE)
summary(NEWdataML2019)
#no 2020
summary(NEWdataML2021)
summary(NEWdataML2022)
#identifying missing driver data - just to know
#2019
str(NEWdataML2019)
NEWdataML2019_DOC<-NEWdataML2019[,c(19,20)]
head(NEWdataML2019_DOC)
missing_rows <- NEWdataML2019_DOC[apply(NEWdataML2019_DOC, 1, function(x) any(is.na(x))), ]
print(missing_rows)
#2021
str(NEWdataML2021)
NEWdataML2021_DOC<-NEWdataML2021[,c(19,20)]
head(NEWdataML2021_DOC)
missing_rows <- NEWdataML2021_DOC[apply(NEWdataML2021_DOC, 1, function(x) any(is.na(x))), ]
print(missing_rows)
#2022
NEWdataML2022_DOC<-NEWdataML2022[,c(19,20)]
head(NEWdataML2022_DOC)
missing_rows <- NEWdataML2022_DOC[apply(NEWdataML2022_DOC, 1, function(x) any(is.na(x))), ]
print(missing_rows)
str(NEWdataML2018)
Ndata_F<-NEWdataML2018[,c(19,8,3,16, 18, 9, 14,15)]
names(Ndata_F)
str(Ndata_F)
head(Ndata_F)
#check for correlations
cormat <- cor(Ndata_F %>% keep(is.numeric), use="complete.obs")
cormat %>% as.data.frame %>% mutate(var2=rownames(.)) %>%
pivot_longer(!var2, values_to = "value") %>%
ggplot(aes(x=name,y=var2,fill=abs(value),label=round(value,2))) +
geom_tile() + geom_label() + xlab("") + ylab("") +
ggtitle("Correlation matrix of our predictors") +
labs(fill="Correlation\n(absolute):")
#this is really useful code; can I get rid of the '1's?
highcorr <- which(cormat > .8, arr.ind = T)
paste(rownames(cormat)[row(cormat)[highcorr]],
colnames(cormat)[col(cormat)[highcorr]], sep=" vs. ") %>%
cbind(cormat[highcorr])
#remove NAs in unscaled data
Ndata_F<-Ndata_F[complete.cases(Ndata_F), ]
summary(Ndata_F)
str(Ndata_F)
#remove NAs from all data
Ndata_F_strip<-Ndata_F[complete.cases(Ndata_F), ]
str(Ndata_F_strip)
#drop DOC_est_FEE
drops <- c('DOC_est_FEE')
NEWdata_F_num <-  Ndata_F_strip[ , !(names(Ndata_F_strip) %in% drops)]
str(NEWdata_F_num)
#will then add DOC_est_FEE back in
DOC_est_FEE<-Ndata_F_strip$DOC_est_FEE
#scale
NEWscaled_data_F_num <- scale(NEWdata_F_num)
NEWscaledDF<-as.data.frame(NEWscaled_data_F_num)
str(NEWscaledDF)
NEWscaledDF<- cbind(NEWscaledDF, DOC_est_FEE)
str(NEWscaledDF)
#also remove NAs unscaled data
Ndata_F<-Ndata_F[complete.cases(Ndata_F), ]
summary(Ndata_F)
str(Ndata_F)
#DECISION TREES
# Dividir los datos en conjuntos de entrenamiento y prueba (70% para entrenamiento)
sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
traindata = Ndata_F[sample, ] #just the samples
testdata  = Ndata_F[-sample, ] #everything but the samples
fitrp <- rpart(DOC_est_FEE ~ . -(DOC_est_GG) , data = traindata, cp = 0)
# Identificar el cp óptimo, que minimiza el error de validación cruzada (xerror)
cpopt <- fitrp$cptable[which.min(fitrp$cptable[,"xerror"]),"CP"]; cpopt
#Revisar que error baje:
plot(fitrp$cptable[,"xerror"])
X11(); rpart.plot(fitrp, type = 4) # demasiado grande
# Hacemos predicciones en el conjunto de prueba
pred <- predict(fitrp, testdata)
rsq <- round((cor(pred, testdata$DOC_est_FEE))^2,2); rsq
sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
Ndata_F
dim(Ndata)[1]
dim(Ndata)
dim(Ndata_F)[1]
dim(Ndata_F)
#sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
sample <- 1:(dim(Ndata)[1]*0.8)
#sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
sample <- 1:(dim(Ndata_F)[1]*0.8)
train = Ndata_F[sample, ] #just the samples
test  = Ndata_F[-sample, ] #everything but the samples
#to check
nrow(train)
nrow(test)
nrow(train) + nrow(test) == nrow(Ndata_F)
#define response (y) and drivers
train_y = train[,'DOC_est_FEE']
train_x = train[, names(train) !='DOC_est_FEE']
train_x = train_x[, names(train_x) !='DOC_est_GG']
#Use set.seed for reproducibility
set.seed(213)
rf_model = randomForest(train_x, y = train_y , ntree = 1000, mtry=5, importance = TRUE, do.trace = 100)
plot(rf_model)
#plot importance values
varImpPlot(rf_model,sort = T, main="Variable Importance", n.var=7, pch=19)
oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions
train_mse = mean(as.numeric((oob_prediction - train_y)^2))
oob_rmse = sqrt(train_mse)
oob_rmse
test_y = test[,'DOC_est_FEE']
test_x = test[, names(test) !='DOC_est_FEE']
test_x = test_x[, names(test_x) !='DOC_est_GG']
y_pred = predict(rf_model , test_x)
rsq <- round((cor(y_pred, test_y))^2, 2)
print(paste("R-squared:", rsq))
test_mse = mean(((y_pred - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
#sample <- 1:(dim(Ndata_F)[1]*0.8)
train = Ndata_F[sample, ] #just the samples
test  = Ndata_F[-sample, ] #everything but the samples
#to check
nrow(train)
nrow(test)
nrow(train) + nrow(test) == nrow(Ndata_F)
#define response (y) and drivers
train_y = train[,'DOC_est_FEE']
train_x = train[, names(train) !='DOC_est_FEE']
train_x = train_x[, names(train_x) !='DOC_est_GG']
#Use set.seed for reproducibility
set.seed(213)
rf_model = randomForest(train_x, y = train_y , ntree = 1000, mtry=5, importance = TRUE, do.trace = 100)
plot(rf_model)
#plot importance values
varImpPlot(rf_model,sort = T, main="Variable Importance", n.var=7, pch=19)
oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions
train_mse = mean(as.numeric((oob_prediction - train_y)^2))
oob_rmse = sqrt(train_mse)
oob_rmse
test_y = test[,'DOC_est_FEE']
test_x = test[, names(test) !='DOC_est_FEE']
test_x = test_x[, names(test_x) !='DOC_est_GG']
y_pred = predict(rf_model , test_x)
rsq <- round((cor(y_pred, test_y))^2, 2)
print(paste("R-squared:", rsq))
test_mse = mean(((y_pred - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
test_rmse
print(paste("R-squared:", rsq))
plot(y_pred)
points(test_y, col="red")
test_y
#load drivers
drivers <- read.csv("~/Documents/intoDBP/HydroC_model/data/drivers.csv")
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom"
min_depth <- 0
max_depth <- 5
target <- read.csv(paste0("~/Documents/intoDBP/HydroC_model/data/target_fDOM_",
min_depth,"-",max_depth,".csv"))
target$date <- as.Date(target$date)
#merge all
data <- merge(drivers, target, by="date")
data$yday <- yday(data$date)
#ML Analysis
###############################################################
#Random forest
library(randomForest)
#train RF
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = data, ntree = 500)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, data[tvar]))^2,2) ; rsqOOB
importance_random <- importance(RFfit); importance_random
#stats are quite good, it seems promising
plot(data$date, data[tvar][,1])
points(data$date, predRF, col="red")
#HOWEVER
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
m <- 1:(dim(data)[1]*0.8)
traindata <- data[m,]
testdata <- data[-m,]
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats with OOB data
predRF<- predict(RFfit) # without data, give the prediction with OOB samples
#check resulting stats with OOB data
predRF<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF, testdata[tvar]))^2,2) ; rsqOOB
rsqOOB <- round((cor(predRF, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF)^2))); rmseOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2))); rmse_test
importance_random <- importance(RFfit); importance_random
#now the stats are as good, but they are real (not affected by autocorrelation)
plot(testdata$date, testdata[tvar][,1])
points(testdata$date, predRF, col="red")
#Now let's test having training and testing being sampled randomly,
#they could be autocorrelated in time and increase
#wrongly the predictability
m <- sample.int(n=nrow(data), size=floor(.8*nrow(data)),replace = F)
traindata <- data[m,]
testdata <- data[-m,]
#Now let's test having training and testing being sampled randomly,
#they could be autocorrelated in time and increase wrongly the predictability
#this can be occurring by chance and it's highly probable because the amount of
#samples are limited
m <- sample.int(n=nrow(data), size=floor(.8*nrow(data)),replace = F)
traindata <- data[m,]
testdata <- data[-m,]
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT RIGHT, it should be done with OOB data, but anyways)
predRF<- predict(RFfit, testdata)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
RFfit <- randomForest(formula, data = traindata, ntree = 500)
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, testdata[tvar][,1]))^2,2) ; rsqOOB
names(drivers)
reticulate::repl_python()
