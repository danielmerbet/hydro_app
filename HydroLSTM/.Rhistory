summary(Ndata_F)
str(Ndata_F)
#DECISION TREES
# Dividir los datos en conjuntos de entrenamiento y prueba (70% para entrenamiento)
sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
train = Ndata_F[sample, ] #just the samples
test  = Ndata_F[-sample, ] #everything but the samples
traindata <- Ndata_F[train, ]
Ndata_F
#DECISION TREES
# Dividir los datos en conjuntos de entrenamiento y prueba (70% para entrenamiento)
sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
train = Ndata_F[sample, ] #just the samples
test  = Ndata_F[-sample, ] #everything but the samples
#DECISION TREES
# Dividir los datos en conjuntos de entrenamiento y prueba (70% para entrenamiento)
sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
traindata = Ndata_F[sample, ] #just the samples
testdata  = Ndata_F[-sample, ] #everything but the samples
fitrp <- rpart(DOC_est_FEE ~ . , data = traindata, cp = 0)
# Identificar el cp óptimo, que minimiza el error de validación cruzada (xerror)
cpopt <- fitrp$cptable[which.min(fitrp$cptable[,"xerror"]),"CP"]; cpopt
#Revisar que error baje:
plot(fitrp$cptable[,"xerror"])
X11(); rpart.plot(fitrp, type = 4) # demasiado grande
fitrp <- rpart(toc ~ . -date, data = data, cp = 0)
X11(); rpart.plot(fitrp, type = 4) # demasiado grande
names(Ndata_F)
# Hacemos predicciones en el conjunto de prueba
pred <- predict(fitrp, testdata)
rsq <- round((cor(pred, testdata$toc))^2,2); rsq
rsq <- round((cor(pred, testdata$DOC_est_FEE))^2,2); rsq
#Comments
#E Jennings updated 7 Nov 2024 - xploring use of random forest Lough Feeagh for short-term forecasts ~30 days
#this version is cleaned to send to DM
#this script explores the the use of randon forest to make short-term predictions of DOC concentrations in Lough Feeagh (FEE)
#it imports daily data for a range of potential drivers; the response is DOC_est_FEE, which is estimated DOC concentration based on temperatrue corrected mean daily CDOM data from the monitoring station on Feeagh (Marine Institute)
#EJ note to self: the input file was prepared in the script 2018dailyML_prepOct24.R
#data are for the period 1/1/2018 to 31/12/2022 but do contain data gaps
#data source: the CDOM, water temperature and stream discharge data used were provided by Elvira de Eyto (EdE)to EJ 8 March 2021 = raw CDOM were drift corrected; EJ did ToC correction and converted to DOC using equation from Jennings et al. 2020 (Water)
#the mean daily Glenamong (GG) stream discharge data were provided by EdE and Mary Dillane
###########
###########
# data files used
#1. DailyGlenFee_1822.csv; daily data collated by EJ in Oct 24; includes Glenamong (GG) discharge and DOC_est based on CDOM for GG and FEE
##########################################
##########################################
rm(list=ls(all=TRUE))
#Set Time Zone
Sys.setenv(TZ='UTC')
#Load Packages - may not use all - Note I did not delete ones I am not using here
library(lattice)
library(lubridate)
library(zoo)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(reshape2)
library(randomForest)
library(rpart)
library(rpart.plot)
##########################################
##########################################
#import dataset 2018-2022
#note here that there is a large datagap from late 2019 to early 2021 in DOC_est_FEE
#this gap in raw data is for both Feeagh CDOM and SWT
#there are also gaps in drivers - see later
dataML1822<-read.csv('~/Documents/intoDBP/ML_DOC_Feeagh/DailyGlenFee_1822.csv', sep=",")
str(dataML1822)
#format Date - note I call it Date2 - I keep the original Date
dataML1822$Date2 <- as.POSIXct(as.character(dataML1822$Date), format = "%Y-%m-%d")
str(dataML1822)
#yes Date2 = as.POSIXct
#a simple plot
plot(dataML1822$Date2, dataML1822$DOC_est_FEE, type="b", pch = 19, xlab = "", ylab = "estimated DOC mg/L")
#Feeagh estimated DOC data are missing in 2020 - is that correct? yes - checked
#identifying missing driver data - just to know
#subset by year
NEWdataML2018<-subset(dataML1822, year==2018)
NEWdataML2019<-subset(dataML1822, year==2019)
#no2020
NEWdataML2021<-subset(dataML1822, year==2021)
NEWdataML2022<-subset(dataML1822, year==2022)
str(NEWdataML2018)
str(NEWdataML2019)
#n0 2020
str(NEWdataML2021)
str(NEWdataML2022)
summary(NEWdataML2018$DOC_est_FEE)
summary(NEWdataML2019)
#no 2020
summary(NEWdataML2021)
summary(NEWdataML2022)
#identifying missing driver data - just to know
#2019
str(NEWdataML2019)
NEWdataML2019_DOC<-NEWdataML2019[,c(19,20)]
head(NEWdataML2019_DOC)
missing_rows <- NEWdataML2019_DOC[apply(NEWdataML2019_DOC, 1, function(x) any(is.na(x))), ]
print(missing_rows)
#2019 missing from 1 Nov
#2021
str(NEWdataML2021)
NEWdataML2021_DOC<-NEWdataML2021[,c(19,20)]
head(NEWdataML2021_DOC)
missing_rows <- NEWdataML2021_DOC[apply(NEWdataML2021_DOC, 1, function(x) any(is.na(x))), ]
print(missing_rows)
#2021 missing to 9 May
#2022
NEWdataML2022_DOC<-NEWdataML2022[,c(19,20)]
head(NEWdataML2022_DOC)
missing_rows <- NEWdataML2022_DOC[apply(NEWdataML2022_DOC, 1, function(x) any(is.na(x))), ]
print(missing_rows)
#2021 missing from 12 Dec
#there are other smaller gaps but less worried about these
#So I do have forcing data and DOC_est_FEE for the lake for two long periods
#This script uses those data from 2018-2019 as example
##########################################
##########################################
#the first RF model is with test dataset nested in training dataset, 2018 only, Date (Date2) not included
#subset data for selected forcing variables - note do not incluee both SWT (lake ToC) and soil = soil T at 10cm
#[1] "DOC_est_FEE" "rain"     "GG_cumec"    "DOC_est_GG"  "SWT"
#[6] "wdsp"        "smd_pd"      "glorad"
str(NEWdataML2018)
Ndata_F<-NEWdataML2018[,c(19,8,3,16, 18, 9, 14,15)]
names(Ndata_F)
str(Ndata_F)
head(Ndata_F)
#check for correlations
cormat <- cor(Ndata_F %>% keep(is.numeric), use="complete.obs")
cormat %>% as.data.frame %>% mutate(var2=rownames(.)) %>%
pivot_longer(!var2, values_to = "value") %>%
ggplot(aes(x=name,y=var2,fill=abs(value),label=round(value,2))) +
geom_tile() + geom_label() + xlab("") + ylab("") +
ggtitle("Correlation matrix of our predictors") +
labs(fill="Correlation\n(absolute):")
#only problem if great than 0.8
#SWT and soil = 0.9
#GG_cumec and rain mm (rainfall from catchment gauges) 0.82; swapped for Furnace (Met Eireann) data
#If you have many, many features and don’t want to look at 1,000 by 1,000 correlation matrices,
#you can then print a list of all correlations that are greater than, say, 0.8 with the following code:
#this is really useful code; can I get rid of the '1's?
highcorr <- which(cormat > .8, arr.ind = T)
paste(rownames(cormat)[row(cormat)[highcorr]],
colnames(cormat)[col(cormat)[highcorr]], sep=" vs. ") %>%
cbind(cormat[highcorr])
#remove NAs in unscaled data
Ndata_F<-Ndata_F[complete.cases(Ndata_F), ]
summary(Ndata_F)
str(Ndata_F)
##########################################
##########################################
#scaling driver data - I will try using scaled data and unscaled
#remove NAs from all data
Ndata_F_strip<-Ndata_F[complete.cases(Ndata_F), ]
str(Ndata_F_strip)
#drop DOC_est_FEE
drops <- c('DOC_est_FEE')
NEWdata_F_num <-  Ndata_F_strip[ , !(names(Ndata_F_strip) %in% drops)]
str(NEWdata_F_num)
#will then add DOC_est_FEE back in
DOC_est_FEE<-Ndata_F_strip$DOC_est_FEE
#scale
NEWscaled_data_F_num <- scale(NEWdata_F_num)
NEWscaledDF<-as.data.frame(NEWscaled_data_F_num)
str(NEWscaledDF)
NEWscaledDF<- cbind(NEWscaledDF, DOC_est_FEE)
str(NEWscaledDF)
#also remove NAs unscaled data
Ndata_F<-Ndata_F[complete.cases(Ndata_F), ]
summary(Ndata_F)
str(Ndata_F)
#DECISION TREES
# Dividir los datos en conjuntos de entrenamiento y prueba (70% para entrenamiento)
sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
traindata = Ndata_F[sample, ] #just the samples
testdata  = Ndata_F[-sample, ] #everything but the samples
# b) Entrenar un árbol de regresión utilizando las variables ambientales como
#    predictoras e identificar la complejidad óptima con la tabla de complejidad (cptable) generada por rpart (10-folds CV).
fitrp <- rpart(DOC_est_FEE ~ . , data = traindata, cp = 0)
# Identificar el cp óptimo, que minimiza el error de validación cruzada (xerror)
cpopt <- fitrp$cptable[which.min(fitrp$cptable[,"xerror"]),"CP"]; cpopt
#Revisar que error baje:
plot(fitrp$cptable[,"xerror"])
X11(); rpart.plot(fitrp, type = 4) # demasiado grande
# Hacemos predicciones en el conjunto de prueba
pred <- predict(fitrp, testdata)
rsq <- round((cor(pred, testdata$DOC_est_FEE))^2,2); rsq
########################
#Section 1 : try RF on UNSCALED and SCALED data = Ndata_F excluding date
#########################
#note here test data are embedded in train
sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
train = Ndata_F[sample, ] #just the samples
test  = Ndata_F[-sample, ] #everything but the samples
#to check
nrow(train)
nrow(test)
nrow(train) + nrow(test) == nrow(Ndata_F)
#define response (y) and drivers
train_y = train[,'DOC_est_FEE']
train_x = train[, names(train) !='DOC_est_FEE']
#Use set.seed for reproducibility
set.seed(213)
#rf_model
#verbose mode(do.trace)
#1 and 2 in the output give the classification error for each class. The OOB value is the weighted average of the class errors (weighted by the fraction of observations in each class).
rf_model = randomForest(train_x, y = train_y , ntree = 1000, mtry=5, importance = TRUE, do.trace = 100)
plot(rf_model)
#plot importance values
varImpPlot(rf_model,sort = T, main="Variable Importance", n.var=7, pch=19)
oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions
train_mse = mean(as.numeric((oob_prediction - train_y)^2))
oob_rmse = sqrt(train_mse)
oob_rmse
test_y = test[,'DOC_est_FEE']
test_x = test[, names(test) !='DOC_est_FEE']
y_pred = predict(rf_model , test_x)
rsq <- round((cor(y_pred, test_y))^2, 2)
print(paste("R-squared:", rsq))
test_mse = mean(((y_pred - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
#XGBoosting
library(ggplot2)
library(xgboost)
#sampling with the same data
m <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
traindata <- Ndata_F[m,]
testdata <- Ndata_F[-m,]
#select predictors (x) and target variable (y)
traindatax <- as.matrix(subset(traindata, select=-c(DOC_est_FEE)))
traindatay <- traindata$DOC_est_FEE
#convert data to xgb.DMatrix
dtrain <- xgb.DMatrix(data = traindatax, label = traindatay)
testdatax<- as.matrix(subset(testdata, select=-c(DOC_est_FEE)))
testdatay<- c(testdata$DOC_est_FEE)
dtest <- xgb.DMatrix(data = testdatax, label = testdatay)
#trainig the model to get best parameters
fitxgb <- xgb.cv(data = dtrain, nrounds = 1000, nfold = 5, verbose = FALSE,
early_stopping_rounds = 50, eval_metric = c("rmse"),
prediction = TRUE)
names(fitxgb)
head(fitxgb$evaluation_log,30) # evolution of errors
#get the parameter with minimum error
minrmse   = min(fitxgb$evaluation_log$test_rmse_mean); minrmse
optntrees  = which.min(fitxgb$evaluation_log$test_rmse_mean); optntrees # con hpps por defecto
#final model with optimum parameters
final_model <- xgboost(data = dtrain,
nrounds = optntrees,
objective = "reg:squarederror",
eval_metric = "rmse",
verbose = 0)
View(Ndata_F)
View(traindata)
reticulate::repl_python()
library(lattice)
library(lubridate)
library(zoo)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(reshape2)
library(randomForest)
library(rpart)
library(rpart.plot)
data <- read.csv("~/Downloads/NET_final.csv")
data <- data[,-c(1:4, 17:24)]
#DECISION TREES
# Dividir los datos en conjuntos de entrenamiento y prueba (70% para entrenamiento)
sample <- sample.int(n = nrow(data), size = floor(.8*nrow(data)), replace = F)
traindata = data[sample, ] #just the samples
testdata  = data[-sample, ] #everything but the samples
fitrp <- rpart(NET ~ . , data = traindata, cp = 0)
# Identificar el cp óptimo, que minimiza el error de validación cruzada (xerror)
cpopt <- fitrp$cptable[which.min(fitrp$cptable[,"xerror"]),"CP"]; cpopt
fitrp <- rpart(NET ~ . , data = traindata, cp = cpopt)
#Revisar que error baje:
plot(fitrp$cptable[,"xerror"])
X11(); rpart.plot(fitrp, type = 4) # demasiado grande
# Hacemos predicciones en el conjunto de prueba
pred <- predict(fitrp, testdata)
rsq <- round((cor(pred, testdata$NET, na.rm=T))^2,2); rsq
# Hacemos predicciones en el conjunto de prueba
pred <- predict(fitrp, testdata)
valid_indices <- !is.na(pred) & !is.na(testdata$NET)
rsq <- round((cor(pred[valid_indices], testdata$NET[valid_indices]))^2, 2)
rsq
# Dividir los datos en conjuntos de entrenamiento y prueba (70% para entrenamiento)
sample <- sample.int(n = nrow(data), size = floor(.8*nrow(data)), replace = F)
traindata = data[sample, ] #just the samples
testdata  = data[-sample, ] #everything but the samples
# b) Entrenar un árbol de regresión utilizando las variables ambientales como
#    predictoras e identificar la complejidad óptima con la tabla de complejidad (cptable) generada por rpart (10-folds CV).
fitrp <- rpart(NET ~ . , data = traindata, cp = 0)
# Identificar el cp óptimo, que minimiza el error de validación cruzada (xerror)
cpopt <- fitrp$cptable[which.min(fitrp$cptable[,"xerror"]),"CP"]; cpopt
fitrp <- rpart(NET ~ . , data = traindata, cp = cpopt)
#Revisar que error baje:
plot(fitrp$cptable[,"xerror"])
X11(); rpart.plot(fitrp, type = 4) # demasiado grande
# Hacemos predicciones en el conjunto de prueba
pred <- predict(fitrp, testdata)
valid_indices <- !is.na(pred) & !is.na(testdata$NET)
rsq <- round((cor(pred[valid_indices], testdata$NET[valid_indices]))^2, 2)
rsq
# Dividir los datos en conjuntos de entrenamiento y prueba (70% para entrenamiento)
sample <- sample.int(n = nrow(data), size = floor(.8*nrow(data)), replace = F)
traindata = data[sample, ] #just the samples
testdata  = data[-sample, ] #everything but the samples
# b) Entrenar un árbol de regresión utilizando las variables ambientales como
#    predictoras e identificar la complejidad óptima con la tabla de complejidad (cptable) generada por rpart (10-folds CV).
fitrp <- rpart(NET ~ . , data = traindata, cp = 0)
# Identificar el cp óptimo, que minimiza el error de validación cruzada (xerror)
cpopt <- fitrp$cptable[which.min(fitrp$cptable[,"xerror"]),"CP"]; cpopt
fitrp <- rpart(NET ~ . , data = traindata, cp = cpopt)
#Revisar que error baje:
plot(fitrp$cptable[,"xerror"])
X11(); rpart.plot(fitrp, type = 4) # demasiado grande
# Hacemos predicciones en el conjunto de prueba
pred <- predict(fitrp, testdata)
valid_indices <- !is.na(pred) & !is.na(testdata$NET)
rsq <- round((cor(pred[valid_indices], testdata$NET[valid_indices]))^2, 2)
rsq
data
names(data)
#RANDOM FOREST
#sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
#train = Ndata_F[sample, ] #just the samples
#test  = Ndata_F[-sample, ] #everything but the samples
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
X11();plot(RFfit)
library(lattice)
library(lubridate)
library(zoo)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(reshape2)
library(randomForest)
library(rpart)
library(rpart.plot)
data <- read.csv("~/Downloads/NET_final.csv")
data <- data[,-c(1:4, 17:24)]
#RANDOM FOREST
#sample <- sample.int(n = nrow(Ndata_F), size = floor(.8*nrow(Ndata_F)), replace = F)
#train = Ndata_F[sample, ] #just the samples
#test  = Ndata_F[-sample, ] #everything but the samples
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
colSums(is.na(data))
View(data)
data_na <- na.omit(data)
View(data_na)
colSums(is.na(data))
colSums(is.na(data_na))
#RANDOM FOREST
#remove all na
colSums(is.na(data))
colSums(is.na(data))
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
#RANDOM FOREST
#remove all na
colSums(is.na(data))
data <- na.omit(data)
colSums(is.na(data))
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$doc))^2,2) ; rsqOOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); importance_random
plot(data$NET, predRF, type="l")
plot(data$NET, predRF)
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); importance_random
plot(data$NET, predRF)
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); importance_random
plot(data$NET, predRF)
#RANDOM FOREST
#remove all na
colSums(is.na(data))
data <- na.omit(data)
colSums(is.na(data))
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); #importance_random
plot(data$NET, predRF)
colSums(is.na(data))
data <- na.omit(data)
colSums(is.na(data))
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); #importance_random
colSums(is.na(data))
data <- na.omit(data)
colSums(is.na(data))
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); #importance_random
colSums(is.na(data))
data <- na.omit(data)
colSums(is.na(data))
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); #importance_random
colSums(is.na(data))
data <- na.omit(data)
colSums(is.na(data))
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); #importance_random
colSums(is.na(data))
data <- na.omit(data)
colSums(is.na(data))
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); #importance_random
colSums(is.na(data))
data <- na.omit(data)
colSums(is.na(data))
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); #importance_random
colSums(is.na(data))
data <- na.omit(data)
colSums(is.na(data))
RFfit <- randomForest(NET ~ . , data = data, ntree = 500)
#X11();plot(RFfit)
ntree <- RFfit$ntree; ntree
#check resulting stats (NOT GOOD, it should be done with OOB data, by anyways)
rsq <- (RFfit$rsq)[ntree]; round(rsq, 3)
rmse <- sqrt((RFfit$mse)[ntree]); round(rmse, 3)
#testing: check with data not used in training (Out-of-Bag data)
predRF<- predict (RFfit) # sin data da la predicción sobre las muestras OOB
rsqOOB = round((cor(predRF, data$NET))^2,2) ; rsqOOB
importance_random <- importance(RFfit); #importance_random
plot(data$NET, predRF)
#soil and meteo data from ICGS
soil_meteo <- read.csv("~/Documents/intoDBP/ICGS_data/soil_meteo_data.csv")
soil_meteo$date <- as.Date(soil_meteo$TmStamp)
reticulate::repl_python()
